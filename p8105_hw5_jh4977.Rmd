---
title: "p8105_hw5_jh4977"
author: "jh4977"
date: "2025-11-12"
output: github_document
---

```{r setup, include=FALSE}
# what has benn down here is to set global plots form.
library(tidyverse)
library(rvest)
library(broom)
library(purrr)
library(tibble)


#knitr is a package, opts_chunk is a set of fun in knitr, $ means sxtract from opts_chunk, to get "set"
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)

```

# **Q1**
Set a function if there are same birthdays, then the length will smaller than n.
```{r}
duplicate_birth = function(n){
  birthdays = sample(1:365, n, replace = TRUE)
  return(length(unique(birthdays)) < n)
}
```

Test
```{r}
duplicate_birth(5)
```

Run 10000 from 2:50, use mean to calculate the probability
```{r}
simulate = function(n, rep = 10000) {
  results = replicate(rep, duplicate_birth(n))
  mean(results)
}
```

```{r}

group = 2:50
prob = sapply(group, simulate)

table = tibble(
  group_size = group,
  probability = prob
)
table

# plot probability with group size
prob_plot = 
  table %>% 
  ggplot(aes(x = group_size, y = probability)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Probability of Shared Birthday by Group Size",
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal()

prob_plot

```
 
### Comment

The plot shows a smooth and rapidly increasing relationship between group size and the probability of at least two people sharing a birthday. When the group is small (fewer than 10 people), the probability stays close to zero. However, the curve rises steeply between group sizes 15 and 30, reflecting how quickly collisions occur once the number of people becomes moderately large. Around a group size of roughly 23, the probability crosses 0.5, the classic birthday paradox point where a shared birthday becomes more likely than not. Beyond 30 people, the probability continues increasing but tapers off as it approaches 1, indicating that duplicates become almost guaranteed in groups of around 50 people.

# **Q2**
### Simulation
Use function to run the whole simulation.
map_df(a, b): run function b for a and map results up.
Use mean() to calculate the Pro of power(Pr(reject H0))
```{r}
sigma <- 5
n <- 30
set_mu <- 1:6
alpha <- 0.05

sim_one_mu <- function(mu) {
  replicate(5000, {
    x <- rnorm(n, mean = mu, sd = sigma)
    t_out <- t.test(x, mu = 0) %>% 
    broom::tidy()
    tibble(
      mu_true = mu,
      mu_hat = mean(x),
      p_val = t_out$p.value
    )
  }, simplify = FALSE) |> bind_rows()
}

sim_results = map_df(set_mu, sim_one_mu)

power_df <- sim_results %>% 
  group_by(mu_true) %>% 
  summarise(power = mean(p_val < alpha))
```

## Plot
```{r}
# Plot 1: Power vs effect size
ggplot(
  power_df, aes(x = mu_true, y = power)
  ) +
  geom_line() + geom_point() +
  labs(
    title = "Power curve for one-sample t-test",
    x = "True mean (μ)",
    y = "Proportion rejected (Power)"
  ) +
  theme_minimal()

# compute average μ-hat overall and conditional on rejection
mu_summary <- sim_results |>
  group_by(mu_true) |>
  summarise(
    mean_mu_hat = mean(mu_hat),
    mean_mu_hat_reject = mean(mu_hat[p_val < alpha])
  )

# Plot 2: mean estimates
ggplot(mu_summary, aes(x = mu_true)) +
  geom_line(aes(y = mean_mu_hat, color = "All samples")) +
  geom_point(aes(y = mean_mu_hat, color = "All samples")) +
  geom_line(aes(y = mean_mu_hat_reject, color = "Rejected only")) +
  geom_point(aes(y = mean_mu_hat_reject, color = "Rejected only")) +
  labs(
    title = "Average μ̂ vs true μ",
    x = "True μ",
    y = "Average estimated μ̂",
    color = "Condition"
  ) +
  theme_minimal()
```



### **(1) Describe the association between effect size and power.**

The power curve shows a clear positive relationship between the true mean \( \mu \) and the power of the one-sample t-test.
When \( \mu \) is near 0, the null hypothesis is rarely rejected, indicating low power because the true effect is small relative to the noise \( \sigma = 5 \).
As \( \mu \) increases, the probability of rejecting the null rises rapidly — around \( \mu = 3 \) the power exceeds 80%, and when \( \mu \ge 4 \), the test almost always detects the effect (power \( \approx 1 \)).
This demonstrates that larger effect sizes lead to higher power, while small effects often go undetected with a fixed sample size \( n = 30 \).



### (2) **No — especially when the true effect (μ) is small.**
**All samples (purple line):**  
The sample mean \( \hat{\mu} \) is an unbiased estimator of the true mean \( \mu \), so when averaging across all simulated samples, the average \( \hat{\mu} \) is approximately equal to \( \mu \). This is why the purple line follows the diagonal.

**Rejected only (yellow line):**  
When we only keep samples where the null hypothesis was rejected (i.e., \( p < 0.05 \)), we are selectively keeping samples that happened to show unusually large effects by random chance. This selection process introduces **selection bias** (often called the *winner’s curse*).  
As a result, the average \( \hat{\mu} \) in these “significant” samples is larger than the true \( \mu \).

**Why the bias decreases with large μ:**  
When the true effect is large, most samples already reject \( H_0 \); there’s little or no selection happening, so the “rejected-only” mean becomes closer to the true \( \mu \) (the two lines converge on the right).

**Conclusion:**  
The average \( \hat{\mu} \) across all samples is unbiased, but the average among significant samples is inflated. This happens because we condition on statistical significance, which overrepresents extreme estimates that pass the rejection threshold.


# Q3
import data and have a glimpse
```{r}
# 1) Import data
wp_url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homi <- readr::read_csv(wp_url)  
glimpse(homi)
```
**Description of the raw data**

The raw dataset, obtained from *The Washington Post*, contains detailed information on homicides in 50 major U.S. cities. It includes **52,179 observations** and **12 variables**, where each row represents a single homicide case.  

Key variables include:
- `uid`: a unique identifier for each homicide case;  
- `reported_date`: the date when the homicide was reported;  
- `victim_first` and `victim_last`: the first and last names of the victim;  
- `victim_race`, `victim_age`, and `victim_sex`: demographic characteristics of the victim;  
- `city` and `state`: the location of the homicide;  
- `lat` and `lon`: geographic coordinates for the homicide location;  
- `disposition`: the case status, indicating whether the homicide was “Closed by arrest,” “Closed without arrest,” or remains “Open/No arrest.”  

The dataset allows for analysis of homicide clearance rates across major U.S. cities by summarizing the total number of homicides and identifying which cases remain unsolved (defined as those marked “Closed without arrest” or “Open/No arrest”).

```{r}
# 2) homicides
# use 'paste0' to combine characters
homi2 <- homi %>% 
  mutate(city_state = paste0(str_to_title(city), ", ", state)) %>% 
  mutate(unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")) #extract the two type in col "disposition"

city_summ <- homi2 %>% 
  group_by(city_state) %>% 
  summarise(
    n_total   = n(),
    n_unsolve = sum(unsolved),
    .groups = "drop"
  )

# 3) Baltimore
balt <- filter(city_summ, city_state == "Baltimore, MD")
balt_res <- prop.test(balt$n_unsolve, balt$n_total) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)

# 4) for all cities
all_ci <- city_summ %>% 
  mutate(
    test = map2(n_unsolve, n_total, ~ prop.test(.x, .y)),
    tidied = map(test, ~ broom::tidy(.x))
  ) %>% 
  unnest(tidied) %>% 
  select(city_state, n_total, n_unsolve, estimate, conf.low, conf.high)
```

```{r homicide_plot, fig.width=8, fig.asp=2.25, out.width="100%"}

# 5) plot
ggplot(
  all_ci |> mutate(city_state = fct_reorder(city_state, estimate)),
  aes(x = city_state, y = estimate)
) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(x = NULL, y = "Proportion unsolved (with 95% CI)",
       title = "Unsolved homicide proportions by city") +
  theme_minimal(base_size = 12)
```


